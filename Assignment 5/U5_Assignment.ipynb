{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Ayse Sude Baki | 12211229 | 25.05.2023, 08:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 5 – Language Modeling with LSTM (Assignment)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> N. Rekabsaz, B. Schäfl, S. Lehner, J. Brandstetter, E. Kobler, M. Abbass, A. Schörgenhumer<br>\n",
    "<b>Date:</b> 16-05-2023\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "<p><p>This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which might require small code modifications. Most/All of the used functions are imported from the file <code>u5_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u5_utils.py</code> need to be installed.</p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.9 (✓)\n",
      "Installed numpy version: 1.21.5 (✓)\n",
      "Installed pandas version: 1.4.4 (✓)\n",
      "Installed PyTorch version: 1.12.1 (✓)\n"
     ]
    }
   ],
   "source": [
    "import u5_utils as u5\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u5.setup_jupyter()\n",
    "\n",
    "# Check minimum versions.\n",
    "u5.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Model Training and Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Data & Dictionary Preperation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Setup the data set using the same parameter settings as in the main exercise notebook but with the changes mentioned below.</li>\n",
    "            <li>Change the batch size in the initial parameters to $64$ and observe its effect on the created batches. Explain how the corpora are transformed into batches.</li>\n",
    "            <li>Use a seed of $23$.</li>\n",
    "            <li>For a specific sequence in <code>val_data_splits</code> (e.g., index $15$), print the corresponding words of its first 25 wordIDs.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Input & output parameters\n",
    "data_path = os.path.join(\"resources\", \"penn\")\n",
    "save_path = \"model.pt\" # path to save the final model\n",
    "\n",
    "# Training & evaluation parameters\n",
    "train_batch_size = 64 # batch size for training\n",
    "eval_batch_size = 64 # batch size for validation/test\n",
    "max_seq_len = 40 # sequence length\n",
    "\n",
    "# Random seed to facilitate reproducibility\n",
    "torch.manual_seed(23)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dictionary 10001\n"
     ]
    }
   ],
   "source": [
    "train_corpus = u5.Corpus(os.path.join(data_path, \"train.txt\"))\n",
    "valid_corpus = u5.Corpus(os.path.join(data_path, \"valid.txt\"))\n",
    "test_corpus = u5.Corpus(os.path.join(data_path, \"test.txt\"))\n",
    "\n",
    "dictionary = u5.Dictionary()\n",
    "train_corpus.fill_dictionary(dictionary)\n",
    "ntokens = len(dictionary)\n",
    "print(f\"Number of tokens in dictionary {ntokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: number of tokens 929589\n",
      "Validation data: number of tokens 73760\n",
      "Test data: number of tokens 82430\n",
      "\n",
      "Train data split shape: torch.Size([14524, 64])\n",
      "Validation data split shape: torch.Size([1152, 64])\n",
      "Test data batchified shape: torch.Size([1287, 64])\n"
     ]
    }
   ],
   "source": [
    "train_data = train_corpus.words_to_ids(dictionary)\n",
    "print(f\"Train data: number of tokens {len(train_data)}\")\n",
    "\n",
    "valid_data = valid_corpus.words_to_ids(dictionary)\n",
    "print(f\"Validation data: number of tokens {len(valid_data)}\")\n",
    "\n",
    "test_data = test_corpus.words_to_ids(dictionary)\n",
    "print(f\"Test data: number of tokens {len(test_data)}\")\n",
    "\n",
    "print()\n",
    "train_data_splits = u5.batchify(train_data, train_batch_size, device)\n",
    "print(f\"Train data split shape: {train_data_splits.shape}\")\n",
    "\n",
    "val_data_splits = u5.batchify(valid_data, eval_batch_size, device)\n",
    "print(f\"Validation data split shape: {val_data_splits.shape}\")\n",
    "\n",
    "test_data_splits = u5.batchify(test_data, eval_batch_size, device)\n",
    "print(f\"Test data batchified shape: {test_data_splits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words get transformed into word-IDs using a dictionary and sequense of the said IDs get reshaped into batches of the sizes (sequence_length, batch_size). If the sequence_length isn't the multiple of batch_size, the remaining samples are dropped.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weekly\n",
      "reports\n",
      "on\n",
      "school\n",
      "and\n",
      "college\n",
      "construction\n",
      "plans\n",
      "<eos>\n",
      "market\n",
      "data\n",
      "<unk>\n",
      "is\n",
      "a\n",
      "<unk>\n",
      "of\n",
      "educational\n",
      "information\n",
      "and\n",
      "provides\n",
      "related\n",
      "services\n",
      "<eos>\n",
      "closely\n",
      "held\n"
     ]
    }
   ],
   "source": [
    "for idx in val_data_splits[:25,15]:\n",
    "    print(dictionary.idx2word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Copy the implementation of <code>LM_LSTMModel</code> from the main exercise notebook but make the following changes:</li>\n",
    "            <ul>\n",
    "                <li>Add an integer parameter to <code>LM_LSTMModel</code>'s initialization, called <code>num_layers</code> which indicates the number of (vertically) stacked LSTM blocks. Hint: PyTorch's LSTM implementation directly supports this, so you simply have to set it when creating the LSTM instance (see parameter <code>num_layers</code> in the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">documentation</a>).</li>\n",
    "                <li>Add a new bool parameter to <code>LM_LSTMModel</code>'s initialization, called <code>tie_weights</code>. Extend the implementation of <code>LM_LSTMModel</code> such that if <code>tie_weights</code> is set to <code>True</code>, the model ties/shares the parameters of <code>encoder</code> with the ones of <code>decoder</code>. Consider that <code>encoder</code> and <code>decoder</code> still remain separate components but their parameters are now the same (shared). This process is called <i>weight tying</i>. Feel free to search the internet for relevant resources and implementation hints.</li>\n",
    "            </ul>\n",
    "            <li>Create four models:</li>\n",
    "            <ul>\n",
    "                <li>1 layer and without weight tying</li>\n",
    "                <li>1 layer and with weight tying</li>\n",
    "                <li>2 layers and without weight tying</li>\n",
    "                <li>2 layers and with weight tying</li>\n",
    "            </ul>\n",
    "            <li>Compare the number of parameters of the models and report your observations.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_LSTMModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhid, num_layers, tie_weights = False):\n",
    "        super().__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp)  # matrix E in the figure\n",
    "        self.rnn = torch.nn.LSTM(ninp, nhid, num_layers=num_layers)  # Set num_layers in LSTM\n",
    "        self.decoder = torch.nn.Linear(nhid, ntoken)  # matrix U in the figure\n",
    "        self.init_weights()\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.num_layers = num_layers\n",
    "        self.tie_weights = tie_weights\n",
    "        \n",
    "        if tie_weights:\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "            \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(1, bsz, self.nhid),\n",
    "                weight.new_zeros(1, bsz, self.nhid))\n",
    "    \n",
    "    def forward(self, input, hidden=None, return_logs=True):\n",
    "        #ipdb.set_trace()\n",
    "        emb = self.encoder(input)\n",
    "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        decoded = self.decoder(hiddens)\n",
    "        if return_logs:\n",
    "            y_hat = torch.nn.LogSoftmax(dim=-1)(decoded)\n",
    "        else:\n",
    "            y_hat = torch.nn.Softmax(dim=-1)(decoded)\n",
    "        \n",
    "        return y_hat, last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 without weight tying: LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model 1 without weight tying total parameters: 4332001\n",
      "Model 1 without weight tying total trainable parameters: 4332001\n",
      "Model 1 with weight tying total parameters: 2331801\n",
      "Model 1 with weight tying total trainable parameters: 2331801\n",
      "Model 2 without weight tying: LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model 2 without weight tying total parameters: 4653601\n",
      "Model 2 without weight tying total trainable parameters: 4653601\n",
      "Model 2 with weight tying total parameters: 2653401\n",
      "Model 2 with weight tying total trainable parameters: 2653401\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "emsize = 200  # size of word embeddings\n",
    "nhid = 200  # number of hidden units per layer\n",
    "\n",
    "model_1 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=1, tie_weights = False)\n",
    "model_1.to(device)\n",
    "\n",
    "print(f\"Model 1 without weight tying: {model_1}\")\n",
    "print(f\"Model 1 without weight tying total parameters: {sum(p.numel() for p in model_1.parameters())}\")\n",
    "print(f\"Model 1 without weight tying total trainable parameters: {sum(p.numel() for p in model_1.parameters() if p.requires_grad)}\")\n",
    "\n",
    "model_1_wt = LM_LSTMModel(ntokens, emsize, nhid, num_layers=1, tie_weights = True)\n",
    "model_1_wt.to(device)\n",
    "\n",
    "print(f\"Model 1 with weight tying total parameters: {sum(p.numel() for p in model_1_wt.parameters())}\")\n",
    "print(f\"Model 1 with weight tying total trainable parameters: {sum(p.numel() for p in model_1_wt.parameters() if p.requires_grad)}\")\n",
    "\n",
    "\n",
    "model_2 = LM_LSTMModel(ntokens, emsize, nhid, num_layers=2, tie_weights = False)\n",
    "model_2.to(device)\n",
    "\n",
    "print(f\"Model 2 without weight tying: {model_2}\")\n",
    "print(f\"Model 2 without weight tying total parameters: {sum(p.numel() for p in model_2.parameters())}\")\n",
    "print(f\"Model 2 without weight tying total trainable parameters: {sum(p.numel() for p in model_2.parameters() if p.requires_grad)}\")\n",
    "\n",
    "model_2_wt = LM_LSTMModel(ntokens, emsize, nhid, num_layers=2, tie_weights = True)\n",
    "model_2_wt.to(device)\n",
    "\n",
    "print(f\"Model 2 with weight tying total parameters: {sum(p.numel() for p in model_2_wt.parameters())}\")\n",
    "print(f\"Model 2 with weight tying total trainable parameters: {sum(p.numel() for p in model_2_wt.parameters() if p.requires_grad)}\")\n",
    "\n",
    "models=[model_1, model_1_wt, model_2, model_2_wt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight tying reduces the total number of parameters in the model by sharing the encoder and decoder weights. In both models with weight tying, the total number of parameters is significantly lower compared to the models without weight tying, while the number of trainable parameters remains the same. This reduction in parameters can improve memory and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Training and Evaluation</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3. [30 Points]</b>\n",
    "    <ul>\n",
    "        <li>Using the same setup as in the main lecture/exercise notebook, train all four models for $5$ epochs.</li>\n",
    "        <li>Using <code>ipdb</code>, look inside the <code>forward</code> function of <code>LM_LSTMModel</code> during training. Check the forward process from input to output particularly by looking at the shapes of tensors. Report the shape of all tensors used in <code>forward</code>. Try to translate the numbers into batches $B$ and sequence length $L$. For instance, if we know that the batch size is $B=32$, a tensor of shape $(32, 128, 3)$ can be interpreted as a batch of $32$ sequences with $3$ channels of size $L=128$. Thus, this tensor can be translated into $(32, 128, 3) \\rightarrow (B, L, 3)$. Look at the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">official documentation</a> to understand the order of the dimensions.</li>\n",
    "        <li>Evaluate the models. Compare the performances of all four models on the train, validation and test set (for the test set, use the best model according to the respective validation set performance), and report your observations. To do so, create a plot showing the following curves:</li>\n",
    "        <ul>\n",
    "            <li>Loss on each current training batch before every model update step as function of epochs</li>\n",
    "            <li>Loss on the validation set at every epoch</li>\n",
    "        </ul>\n",
    "        <li>Comment on the results!</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT_AFTER_BATCHES = 100  # JUST FOR DEBUGGING: cut the loop after these number of batches. Set to -1 to ignore\n",
    "\n",
    "\n",
    "def plot_losses(model_losses, model_names):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for model_loss, model_name in zip(model_losses, model_names):\n",
    "        plt.plot(model_loss[0], label=f'{model_name} Training Loss')\n",
    "        plt.plot(model_loss[1], label=f'{model_name} Validation Loss')\n",
    "\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def get_batch(data, i, seq_len):\n",
    "    \"\"\"\n",
    "    Get a batch of input data and targets from the given data starting at index i.\n",
    "\n",
    "    Args:\n",
    "        data: Input data tensor.\n",
    "        i: Starting index.\n",
    "        seq_len: Sequence length.\n",
    "\n",
    "    Returns:\n",
    "        batch_data: Batch of input data.\n",
    "        batch_targets: Batch of target data.\n",
    "    \"\"\"\n",
    "    batch_data = data[i:i+seq_len]\n",
    "    batch_targets = data[i+1:i+seq_len+1].view(-1)\n",
    "    return batch_data, batch_targets\n",
    "\n",
    "def repackage_hidden(hidden):\n",
    "    \"\"\"\n",
    "    Detach the hidden state from the computational graph.\n",
    "    \"\"\"\n",
    "    if isinstance(hidden, torch.Tensor):\n",
    "        return hidden.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(h) for h in hidden)\n",
    "\n",
    "def evaluate(model, dictionary, max_seq_len, eval_batch_size, eval_data_splits):\n",
    "    \"\"\"\n",
    "    Evaluate the model. Evaluation mode turned on to disable dropout.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = None\n",
    "    n_batches = (eval_data_splits.size(0) - 1) // max_seq_len\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data_splits.size(0) - 1, max_seq_len):\n",
    "            batch_data, batch_targets = get_batch(eval_data_splits, i, max_seq_len)\n",
    "\n",
    "            if start_hidden is not None:\n",
    "                start_hidden = repackage_hidden(start_hidden)\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat_logprobs, last_hidden = model(batch_data, start_hidden, return_logs=True)\n",
    "\n",
    "            y_hat_logprobs = y_hat_logprobs.view(-1, ntokens)\n",
    "            loss = criterion(y_hat_logprobs, batch_targets.view(-1))\n",
    "\n",
    "            start_hidden = last_hidden\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / n_batches\n",
    "\n",
    "def evaluate(model, criterion, dictionary, max_seq_len, eval_batch_size, eval_data_splits):\n",
    "    \"\"\"\n",
    "    Evaluate the model. Evaluation mode turned on to disable dropout.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = None\n",
    "    n_batches = (eval_data_splits.size(0) - 1) // max_seq_len\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data_splits.size(0) - 1, max_seq_len):\n",
    "            batch_data, batch_targets = get_batch(eval_data_splits, i, max_seq_len)\n",
    "\n",
    "            if start_hidden is not None:\n",
    "                start_hidden = repackage_hidden(start_hidden)\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat_logprobs, last_hidden = model(batch_data, start_hidden, return_logs=True)\n",
    "\n",
    "            y_hat_logprobs = y_hat_logprobs.view(-1, ntokens)\n",
    "            loss = criterion(y_hat_logprobs, batch_targets.view(-1))\n",
    "\n",
    "            start_hidden = last_hidden\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / n_batches\n",
    "\n",
    "def train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits,\n",
    "          clipping, learning_rate, print_interval, epoch, criterion=torch.nn.NLLLoss()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train the model. Training mode turned on to enable dropout.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = None\n",
    "    n_batches = (train_data_splits.size(0) - 1) // max_seq_len\n",
    "\n",
    "    for batch_i, i in enumerate(range(0, train_data_splits.size(0) - 1, max_seq_len)):\n",
    "        batch_data, batch_targets = get_batch(train_data_splits, i, max_seq_len)\n",
    "\n",
    "        # Don't forget it! Otherwise, the gradients are summed together!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Repackaging batches only keeps the value of start_hidden and disconnects its computational graph.\n",
    "        # If repackaging is not done the, gradients are calculated from the current point to the beginning\n",
    "        # of the sequence which becomes computationally too expensive.\n",
    "        if start_hidden is not None:\n",
    "            start_hidden = repackage_hidden(start_hidden)\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat_logprobs, last_hidden = model(batch_data, start_hidden, return_logs=True)\n",
    "\n",
    "        # Loss computation & backward pass\n",
    "        y_hat_logprobs = y_hat_logprobs.view(-1, ntokens)\n",
    "        loss = criterion(y_hat_logprobs, batch_targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # The last hidden states of the current step is set as the start hidden state of the next step.\n",
    "        # This passes the information of the current batch to the next batch.\n",
    "        start_hidden = last_hidden\n",
    "\n",
    "        # Clipping gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "\n",
    "        # Updating parameters using SGD\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_i % print_interval == 0 and batch_i > 0:\n",
    "            cur_loss = total_loss / print_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            throughput = elapsed * 1000 / print_interval\n",
    "            print(f\"| epoch {epoch:3d} | {batch_i:5d}/{n_batches:5d} batches | lr {learning_rate:02.2f} | ms/batch {throughput:5.2f} \"\n",
    "                  f\"| loss {cur_loss:5.2f} | perplexity {math.exp(cur_loss):8.2f}\")\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Cuts the loop (only for debugging)\n",
    "        if (CUT_AFTER_BATCHES != -1) and (batch_i >= CUT_AFTER_BATCHES):\n",
    "            print(f\"WARNING: Training is interrupted after {batch_i} batches\")\n",
    "            break\n",
    "\n",
    "\n",
    "model_losses = []\n",
    "model_names = [\"model_1\", \"model_1_wt\", \"model_2\", \"model_2_wt\"]\n",
    "\n",
    "for model in models:\n",
    "    epochs = 5  # total number of training epochs\n",
    "    print_interval = 25  # print report statistics every x batches\n",
    "    lr = 20  # initial learning rate\n",
    "    clipping = 0.25  # gradient clipping\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = None\n",
    "\n",
    "    # Loop over epochs.\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "        val_loss = evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "              f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            torch.save(model, save_path)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "            for g in optimizer.param_groups:\n",
    "                g[\"lr\"] = lr\n",
    "\n",
    "    model_losses.append((train_losses, val_losses))\n",
    "\n",
    "plot_losses(model_losses, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Language Generation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4. [30 Points]</b>\n",
    "    <p>\n",
    "    Copy the language generation code from the main exercise notebook and perform the following tasks:\n",
    "    </p>\n",
    "        <ul>\n",
    "            <li>Compare all four previous models by generating $12$ words that append the starting word <tt>\"despite\"</tt>.</li>\n",
    "            <li>For each model, retrieve the top $10$ wordIDs with the highest probabilities from the generated probability distribution (<code>prob_dist</code>) following the starting word <tt>\"despite\"</tt>. Fetch the corresponding words of these wordIDs. Do you observe any specific linguistic characteristic common between these words?</li>\n",
    "            <li>The implementation in the main exercise notebook is based on sampling. Implement a second deterministic variant based on the <i>top-1</i> approach. In this particular variant, the generated word is the word with the highest probability in the predicted probability distribution. Repeat the same procedure as before (i.e., generate $12$ words that append the starting word <tt>\"despite\"</tt>).</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Generated text (probabilistic): despite policyholders bankrupt solution oas countries seeing grasp angered heights cleveland district theoretical\n",
      "Top 10 words with the highest probabilities: ['<eos>', '<unk>', 'and', 'of', 'the', 'a', 'in', 'to', 'that', 'is']\n",
      "Generated text (deterministic): despite the <unk> of the <unk> of the <unk> of the <unk> of\n",
      "\n",
      "Model 2\n",
      "Generated text (probabilistic): despite unhappy good bottling indicted outcry seabrook claimants agreements been patch scotland bit\n",
      "Top 10 words with the highest probabilities: ['insure', 'dial', 'yankee', 'century', 'dunes', 'undersecretary', 'accomplished', 'hewlett-packard', 'purchase', 'closings']\n",
      "Generated text (deterministic): despite matthews dial dial dial dial dial dial dial dial dial dial dial\n",
      "\n",
      "Model 3\n",
      "Generated text (probabilistic): despite returning judgment book transatlantic round farmer die '80s redford instrumental legislature respective\n",
      "Top 10 words with the highest probabilities: ['sociologist', 'fan', 'aides', 'smart', 'dollar', 'insisting', 'cleaner', 'supporting', 'fifth', 'passed']\n",
      "Generated text (deterministic): despite supporting supporting fan smart smart fan fan fan fan fan fan fan\n",
      "\n",
      "Model 4\n",
      "Generated text (probabilistic): despite marketer redeem soo means midmorning korotich northern indian bartlett barry depend mlx\n",
      "Top 10 words with the highest probabilities: ['ncnb', 'snack-food', 'morishita', 'committees', 'illegal', 'disposable', 'month', 'grown', 'prison', 'nashua']\n",
      "Generated text (deterministic): despite vancouver committees ncnb ncnb ncnb ncnb ncnb ncnb ncnb ncnb ncnb ncnb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GENERATION_LENGTH = 12\n",
    "START_WORD = \"despite\"\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"Model {model_idx + 1}\")\n",
    "    \n",
    "    start_hidden = None\n",
    "    wordid_input = dictionary.word2idx[START_WORD.lower()]\n",
    "    generated_text = START_WORD\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(GENERATION_LENGTH):\n",
    "            data = torch.tensor([wordid_input]).unsqueeze(1).to(device)\n",
    "            \n",
    "            y_hat_probs, last_hidden = model(data, start_hidden)\n",
    "            prob_dist = torch.distributions.Categorical(y_hat_probs.squeeze())\n",
    "            wordid_input = prob_dist.sample().item()\n",
    "            \n",
    "            generated_word = dictionary.idx2word[wordid_input]\n",
    "            generated_text += \" \" + generated_word\n",
    "            \n",
    "            start_hidden = last_hidden\n",
    "\n",
    "    print(f\"Generated text (probabilistic): {generated_text}\")\n",
    "\n",
    "    top10_wordids = y_hat_probs.squeeze().topk(10).indices.tolist()\n",
    "    top10_words = [dictionary.idx2word[wordid] for wordid in top10_wordids]\n",
    "    print(f\"Top 10 words with the highest probabilities: {top10_words}\")\n",
    "    \n",
    "    start_hidden = None\n",
    "    wordid_input = dictionary.word2idx[START_WORD.lower()]\n",
    "    generated_text = START_WORD\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(GENERATION_LENGTH):\n",
    "            data = torch.tensor([wordid_input]).unsqueeze(1).to(device)\n",
    "            \n",
    "            y_hat_probs, last_hidden = model(data, start_hidden)\n",
    "            wordid_input = y_hat_probs.argmax().item()\n",
    "            \n",
    "            generated_word = dictionary.idx2word[wordid_input]\n",
    "            generated_text += \" \" + generated_word\n",
    "            \n",
    "            start_hidden = last_hidden\n",
    "\n",
    "    print(f\"Generated text (deterministic): {generated_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: Basic function words and determiners such as 'to', 'in', 'and', 'of', 'a', 'the', 'that', \"'s\".\n",
    "Model 2: Diverse range of nouns, verbs, and modifiers representing various concepts and actions.\n",
    "Model 3: Mix of nouns, adjectives, and verbs related to intelligence, support, and specific roles or actions.\n",
    "Model 4: Range of concepts including organizations, food, legal terms, time-related words, and specific locations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
